<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>formal on Qifan Zhang</title><link>https://zh-qifan.github.io/categories/formal/</link><description>Recent content in formal on Qifan Zhang</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 08 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://zh-qifan.github.io/categories/formal/index.xml" rel="self" type="application/rss+xml"/><item><title>Why does Jackknife method works?</title><link>https://zh-qifan.github.io/en/2022/10/08/why-jackknife-work/</link><pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate><guid>https://zh-qifan.github.io/en/2022/10/08/why-jackknife-work/</guid><description>In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows, \begin{equation} b_{JACK} = (n - 1)(\bar{T}_{n-1} - T_n) \end{equation} where $\bar{T}_{n-1} = n^{-1}\sum_{i=1}^n T_{n-1,i} = n^{-1}\sum_{i=1}^n T_{n-1}(X_1,\cdots,X_{i-1},X_{i+1},\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset.</description></item></channel></rss>