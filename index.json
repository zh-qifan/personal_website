[{"categories":["math"],"content":"Jordan-Hahn decomposition must be one of the most important theorem in measure theory but its proof is both lengthy and hard to understand, at least for beginners. In this post, I want to give an intuitive understanding on its proof to illustrate why it constructs a set like that and what it is doing. Jordan-Hahn decomposition states as follows, Let $\\nu$ be a signed measure on $(\\Omega, \\mathcal{F})$. Then it has a decomposition \\begin{equation}\\nu = \\nu^+ - \\nu^- \\end{equation} where \\begin{equation} \\nu^+(A) = \\sup \\{\\nu(B)|B\\subset A, B \\in \\mathcal{F}\\} \\end{equation}\\begin{equation} \\nu^-(A) = \\sup \\{-\\nu(B)|B\\subset A, B \\in \\mathcal{F}\\} \\end{equation} and both $\\nu^+$ and $\\nu^-$ are measure and one of them is a finite measure. Furthermore, there exists a set $D\\in \\mathcal{F}$ s.t. \\begin{equation} \\nu^+(A) = \\nu(A\\cap D),\\ \\nu^-(A) = \\nu(A\\cap D^c) \\end{equation} Before we move on to its proof, let’s consider a special case - indefinite integration. Let $f$ be a measurable function on $(\\Omega, \\mathcal{F}, \\mu)$ and define the $\\nu$ as follows \\begin{equation} \\nu(A) = \\int_{A} f d\\mu \\end{equation} Clearly, it is a signed measure and it has a natural decomposition \\begin{equation} \\nu(A) = \\int_{A} f^+ d\\mu - \\int_{A} f^- d\\mu \\end{equation} Now let’s check whether the definition of $\\nu^+$ and $\\nu^-$ in the Jordan-Hahn theorem can be applied to this case. Consider an arbitrary $A\\in \\mathcal{F}$ and $\\nu^+(A)$. Clearly, the following figure illustrates that $\\nu^+(A) = \\int_{A} f^+ d\\mu$ in this case, where $\\nu(B) = \\nu^+(A)$. So we may assume that there may be an “underlying $f$” for every signed measure $\\nu$ (In fact, this is not true by Lebesgue decomposition). What we need to do now is to find the $f^+$ without mentioning $f$, and to find the $f^+$, we just need to find its “support” i.e. those sets in $\\mathcal{F}$ where $f^+$ not equal to zero on every point of them since if so, we have $f(x) = f^+(x)$ or $\\nu(A) = \\nu^+(A)$. So how to find these “support” and how to find the biggest one of them? Now we only have one tool to do this - measure (or integration). Note that in the indefinite integration case, we have if $\\forall B \\subset A\\in \\mathcal{F}$, $B\\in \\mathcal{F}$, $\\int_B fd\\mu \\geq 0$, then $f \\geq 0$ on $A$. Here we can translate the integration to measure i.e. if $\\forall B \\subset A\\in \\mathcal{F}$, $B\\in \\mathcal{F}$, $\\nu(B) \\geq 0$, then $f \\geq 0$. The picture is clear now. We just need to find those set s.t. every subset of it has positive measure. So we define \\begin{equation} \\mathcal{B} = \\{B\\in \\mathcal{F} | \\forall C\\in \\mathcal{F}, C\\subset B, \\nu(C) \\geq 0\\} = \\{B\\in \\mathcal{F} | \\nu^-(B) = 0\\} \\end{equation} We can be confident to say that this $\\mathcal{B}$ captures all part of “$f^+$”. Our next task is to find the real support of “$f^+$” i.e. a $N\\in\\mathcal{B}$ s.t. $f^+ = 0$ on $N^c$. There are two way to do it: (i) find the set with the biggest measure; (ii) find the biggest set. One can refer to Meature Theory by Dr. Jiaan Yan to see how to do the first one. Here I will use the Zorn Lemma to go through the second one. Define the partial order on $\\mathcal{B}$ to be $\\subset$. Since $\\mathcal{B}$ is closed under union (why?), by Zorn Lemma, we can find the biggest set $D$. So we have answered the two questions mentioned above. Our last task is to show that $\\nu^+(A) = \\nu(A\\cap D)$, which is intuitive based on our discussion above. Before moving on, we need to test whether the $D^c$ capture the support of $f^-$. Intuitively, if there is a $B \\subset D^c$ s.t. $\\nu(B) \u003e 0$, we can show that $\\nu^- (B) \u003e 0$. If not, there will be a contradiction to our construction of $D$, which can be illustrated by the following figure. where the original $D$ and $B$ will constitute a larger $D$. Thus we have $\\nu(B) \u003e 0$ with $\\nu^- (B) \u003e 0$ now, which is clearly a contradiction when $\\nu$ is an indefinite integral. For general cases, this case is like the following case. So we need to de","date":"2022-10-30","objectID":"/posts/intuition-jordan-hahn-thm/:0:0","tags":["article"],"title":"An intuitive understanding of the proof for Jordan-Hahn decomposition","uri":"/posts/intuition-jordan-hahn-thm/"},{"categories":null,"content":"Qifan Zhang Nanjing University Nanjing University Xianlin Campus, Nanjing, China Email: 191840336@smail.nju.edu.cn ","date":"2022-10-16","objectID":"/about/:0:0","tags":null,"title":"Curriculum Vitae","uri":"/about/"},{"categories":null,"content":"Education Nanjing University BS in Statistics | Expected June 2023 National University of Singapore Student Exchange Program | 2021 Aug. - 2021 Dec. ","date":"2022-10-16","objectID":"/about/:1:0","tags":null,"title":"Curriculum Vitae","uri":"/about/"},{"categories":null,"content":"Experience Research Assistant, Singapore Management University Nov. 2021 – Present Scraped data related to job, school, and house location in Singapore using Google APIs and Python. Developed XGBoost/Random Forests/LSTM to predict house price in Singapore. Performed residual analysis and adjusted the criterion for clustering similar houses. Product Data Engineer Summer Intern, Huawei June 2022 – Aug. 2022 Analyzed WIFI connection and App usage data to build social network using SQL/Python. Deployed LightGBM/DNN/LR models to generate persona tags on family role/advertisement click. Optimized data synchronization between different platform like MLOps/DataOps. Presented insights and learnings to leadership teams and in weekly meetings. Quantitative Researcher Intern, Xishi Technologies Aug. 2021 – Nov. 2021 Investigated academic papers in Directional Change (DC) time series framework. Reproduced results of one paper about parameter tuning in DC with Genetic Algorithm using deap. Generated and analyzed 16+ DC-based indicators. Team Captain, Soccer Team in Department of Mathematics, Nanjing University Sept. 2020 – June 2021 Won the championship of Nanjing University Soccer Premier League. ","date":"2022-10-16","objectID":"/about/:2:0","tags":null,"title":"Curriculum Vitae","uri":"/about/"},{"categories":null,"content":"Scholarships \u0026 Awards 2022, Meritorious Winner in Mathematical Contest in Modeling (MCM) 2021, Cheung Kong Graduate School of Business Scholarship 2021, Math Honor Program Scholarship 2021, Alibaba Global Mathematics Competition, Ranked 253 2021, Honorable Mention in Mathematical Contest in Modeling (MCM) 2020, The Chinese Mathematics Competitions, The First Price in Jiangsu Province 2020, Academic Scholarship 2020, China Nantional Scholarship 2020, Men’s 100m bronze medal in Nanjing University Athletics Championship 2020, Men’s 4x100m bronze medal in Nanjing University Athletics Championship ","date":"2022-10-16","objectID":"/about/:3:0","tags":null,"title":"Curriculum Vitae","uri":"/about/"},{"categories":["statistics"],"content":"In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows, \\begin{equation} b_{JACK} = (n - 1)(\\bar{T}_{n-1} - T_n) \\end{equation} where $\\bar{T}_{n-1} = n^{-1}\\sum_{i=1}^n T_{n-1,i} = n^{-1}\\sum_{i=1}^n T_{n-1}(X_1,\\cdots,X_{i-1},X_{i+1},\\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset. This result was beautiful, since we don’t need any additional information even when the true bias of the estimator contains unknow parameter $\\theta$. But there is one question: is this simple bias estimator accurate? This formula was first proposed by Quenouille in 1949 and the motivation was that suppose the bias $b$ has the expansion1 (why?) \\begin{equation} b = \\frac{a_1}{n} + \\frac{a_2}{n^2} + \\cdots \\end{equation} Then we can verify that by subtracting $b_{JACK}$ from the original $T$, we can reduce the estimation bias from $O(n^{-1})$ to $O(n^{-2})$. In fact, if we consider the case where $T_n=g(\\bar{X}_n)$ is used to estimate $\\theta=g(\\mu)$ and $g$ is sufficiently smooth2(e.g. method of moment), we have Taylor expansion \\begin{equation} T_n - \\theta = g^{\\prime}(\\mu) (\\bar{X}_n -\\mu) + \\frac{1}{2}(\\bar{X}_n -\\mu)^Tg^{\\prime\\prime}(\\mu)(\\bar{X}_n -\\mu) + R_n \\end{equation} where $R_n=O_{p}(n^{-2})$3 and $$ \\begin{aligned} E[g^{\\prime}(\\mu) (\\bar{X}_n -\\mu)] \u0026= 0\\\\ E[(\\bar{X}_n -\\mu)^Tg^{\\prime\\prime}(\\mu)(\\bar{X}_n -\\mu)] \u0026= O(n^{-1}) \\end{aligned} $$ Thus in this case the expansion $(2)$ is correct. Let’s further calculate the $b_{JACK}$ in this case. Aagain by Taylor expansion, we have \\begin{equation} T_{n-1, i} - T_n = g^{\\prime}(\\bar{X}_n)(\\bar{X}_{n-1,i} - \\bar{X}_n) + \\frac{1}{2}(\\bar{X}_{n-1,i} - \\bar{X}_n)^Tg^{\\prime\\prime}(\\xi_i)(\\bar{X}_{n-1,i} - \\bar{X}_n) \\end{equation} Then $$ \\begin{aligned} b_{JACK} \u0026= (n - 1)(\\bar{T}_{n-1} - T_n) \\\\ \u0026= \\frac{n-1}{2n}\\sum_{i=1}^n (\\bar{X}_{n-1,i} - \\bar{X}_n)^Tg^{\\prime\\prime}(\\xi_i)(\\bar{X}_{n-1,i} - \\bar{X}_n) \\\\ \u0026= \\frac{1}{2n(n-1)}\\sum_{i=1}^n (X_i - \\bar{X}_n)^Tg^{\\prime\\prime}(\\xi_i)(X_i - \\bar{X}_n) \\end{aligned} $$ where $\\bar{X}_{n-1,i}$ is the mean value of $n-1$ observations without the ith one. Since $\\xi_i \\rightarrow \\mu $ as $n \\rightarrow \\infty$, we can show that $b_{JACK} \\rightarrow E[(\\bar{X}_n -\\mu)^Tg^{\\prime\\prime}(\\mu)(\\bar{X}_n -\\mu)]$. By $E[g^{\\prime}(\\mu) (\\bar{X}_n -\\mu)] = 0$, we actually prove that the jackknife bias estimator is a consistent estimator of the first two order of the true bias. From the above discussion, jackknife bias estimation works well for most estimators i.e. estimators have the form $g(\\bar{X}_n)$ as $n\\rightarrow \\infty$. In fact, for a more general class of statistics named functional statistics4 (e.g. quantile), we have similar result in the consistency of jackknife bias estimation. Jackknife method is popular for its variance estimation, which was first proposed by Tukey in 1958. In fact, the name “jackknife” was first proposed by Tukey because, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool5. By doing jackknife bias reduction, i.e. subtracting bias estimation from the original estimator $T_n$, we have \\begin{equation} T_{JACK} = T_n - (n-1)(\\bar{T}_{n-1} - T_n) = \\frac{1}{n}\\sum_{i=1}^n [nT_n - (n-1)T_{n-1,i}] \\end{equation} This form is informative since if we set $\\tilde{T}_{n,i} = nT_n - (n-1)T_{n-1,i}$, named jackknife pseudovalues, $T_{JACK}$ is the mean value of some new statistics $\\tilde{T}_{n,i}$’s. Tukey conjectured that $\\tilde{T}_{n,i}$’s can be treated as though they were i.i.d. This con","date":"2022-10-08","objectID":"/posts/why-jackknife-work/:0:0","tags":["article"],"title":"Why does Jackknife method works?","uri":"/posts/why-jackknife-work/"},{"categories":["math"],"content":"In abstract measure theory, we have one proposition about the composition of measurable function. Suppose $(\\Omega_1,\\Sigma_1)$, $(\\Omega_2,\\Sigma_2)$ and $(\\Omega_3,\\Sigma_3)$ are measurale spaces, and $f:(\\Omega_1,\\Sigma_1) \\rightarrow (\\Omega_2,\\Sigma_2)$ and $g:(\\Omega_2,\\Sigma_2) \\rightarrow (\\Omega_3,\\Sigma_3)$ are measurable functions. Then $g\\circ f$ is a measurable function from $(\\Omega_1,\\Sigma_1)$ to $(\\Omega_3,\\Sigma_3)$. The proof of this proposition is trivial. But one thing confused me: in Lebesgue measure theory, we have a proposition that composition of a real-valued continuous function and a real-valued measurable function is measurable but the opposite (composition of a measurable function and a continuous function is measurable) is not right. The counterexample can be constructed using Cantor function and Vitali Set. This seems to contradict the above proposition since continuous functions are measurable. Where is the mistake? Let’s review theory on Lebesgue measure to find the mistake. The definition of Lebesgue measurable function is Given a measurable space $(\\Omega, \\Sigma)$ and a topological space $(U,\\tau)$. A function $f$ is measurable if $\\forall A \\in \\tau$, $f^{-1}(A) \\in \\Sigma$. In $\\mathbb{R}$, its topology is the collection of all open interval. Thus the above definition is the same as using Boreal field to define measurable functions from $(\\Omega, \\Sigma)$ to $(\\mathbb{R}, \\mathcal{B(\\mathbb{R})})$. Lebesgue measurable function (one dimension) means the domain of the measurable function is Lebesgue measurable space $(\\mathbb{R}, \\mathcal{L})$. Obviously, a continuous function on $\\mathbb{R}$ is a Lebesgue measurable function. Let’s make it clearer here. Let $f$ be the Lebesgue measurable function and $g$ be the continuous function. Then $$f:(\\mathbb{R}, \\mathcal{L}) \\rightarrow (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$$ $$g:(\\mathbb{R}, \\mathcal{L}) \\rightarrow (\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$$ The mistake is clear now. The $(\\mathbb{R}, \\mathcal{B}(\\mathbb{R}))$ and $(\\mathbb{R}, \\mathcal{L})$ are not the same thing! This does not satisfy the premise of the proposition! Since $\\mathcal{L}$ is a complete $\\sigma$-field, it can hide something nasty into any zero measure set. Then the “normal” $\\mathcal{B}(\\mathbb{R})$ makes it reappear in front of us. This mistake confuse me for about 30 minutes. I think next time I should write down every measurable space I use to avoid problems like this. ","date":"2022-09-29","objectID":"/posts/mistake-composition-measurable-function/:0:0","tags":["article"],"title":"A Mistake on Composition of Measurable Function","uri":"/posts/mistake-composition-measurable-function/"},{"categories":["others"],"content":"This is my very first English Blog, but nothing is here. ","date":"2022-09-28","objectID":"/posts/first-post/:0:0","tags":["article"],"title":"My First Blog","uri":"/posts/first-post/"}]