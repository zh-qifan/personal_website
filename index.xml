<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Qifan Zhang</title><link>https://zh-qifan.github.io/</link><description>Qifan Zhang, a fourth year student in NJU</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 08 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://zh-qifan.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>An intuitive understanding of the proof for Jordan-Hahn decomposition</title><link>https://zh-qifan.github.io/posts/intuition-jordan-hahn-thm/</link><pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate><author>Qifan Zhang</author><guid>https://zh-qifan.github.io/posts/intuition-jordan-hahn-thm/</guid><description>Jordan-Hahn decomposition must be one of the most important theorem in measure theory but its proof is both lengthy and hard to understand, at least for beginners. In this post, I want to give an intuitive understanding on its proof to illustrate why it constructs a set like that and what it is doing.
Jordan-Hahn decomposition states as follows,
Let $\nu$ be a signed measure on $(\Omega, \mathcal{F})$. Then it has a decomposition \begin{equation}\nu = \nu^+ - \nu^- \end{equation} where \begin{equation} \nu^+(A) = \sup \{\nu(B)|B\subset A, B \in \mathcal{F}\} \end{equation}\begin{equation} \nu^-(A) = \sup \{-\nu(B)|B\subset A, B \in \mathcal{F}\} \end{equation} and both $\nu^+$ and $\nu^-$ are measure and one of them is a finite measure.</description></item><item><title>Why does Jackknife method works?</title><link>https://zh-qifan.github.io/posts/why-jackknife-work/</link><pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate><author>Qifan Zhang</author><guid>https://zh-qifan.github.io/posts/why-jackknife-work/</guid><description>In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows, \begin{equation} b_{JACK} = (n - 1)(\bar{T}_{n-1} - T_n) \end{equation} where $\bar{T}_{n-1} = n^{-1}\sum_{i=1}^n T_{n-1,i} = n^{-1}\sum_{i=1}^n T_{n-1}(X_1,\cdots,X_{i-1},X_{i+1},\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset.</description></item><item><title>A Mistake on Composition of Measurable Function</title><link>https://zh-qifan.github.io/posts/mistake-composition-measurable-function/</link><pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate><author>Qifan Zhang</author><guid>https://zh-qifan.github.io/posts/mistake-composition-measurable-function/</guid><description>In abstract measure theory, we have one proposition about the composition of measurable function. Suppose
$(\Omega_1,\Sigma_1)$, $(\Omega_2,\Sigma_2)$ and $(\Omega_3,\Sigma_3)$ are measurale spaces, and $f:(\Omega_1,\Sigma_1) \rightarrow (\Omega_2,\Sigma_2)$ and $g:(\Omega_2,\Sigma_2) \rightarrow (\Omega_3,\Sigma_3)$ are measurable functions. Then $g\circ f$ is a measurable function from $(\Omega_1,\Sigma_1)$ to $(\Omega_3,\Sigma_3)$.
The proof of this proposition is trivial. But one thing confused me: in Lebesgue measure theory, we have a proposition that composition of a real-valued continuous function and a real-valued measurable function is measurable but the opposite (composition of a measurable function and a continuous function is measurable) is not right.</description></item><item><title>My First Blog</title><link>https://zh-qifan.github.io/posts/first-post/</link><pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate><author>Qifan Zhang</author><guid>https://zh-qifan.github.io/posts/first-post/</guid><description>This is my very first English Blog, but nothing is here.</description></item></channel></rss>