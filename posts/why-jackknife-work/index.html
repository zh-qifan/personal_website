<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Why does Jackknife method works? - Qifan Zhang</title><meta name=Description content="Qifan Zhang, a fourth year student in NJU"><meta property="og:title" content="Why does Jackknife method works?"><meta property="og:description" content="In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows, \begin{equation} b_{JACK} = (n - 1)(\bar{T}_{n-1} - T_n) \end{equation} where $\bar{T}_{n-1} = n^{-1}\sum_{i=1}^n T_{n-1,i} = n^{-1}\sum_{i=1}^n T_{n-1}(X_1,\cdots,X_{i-1},X_{i+1},\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset."><meta property="og:type" content="article"><meta property="og:url" content="https://zh-qifan.github.io/posts/why-jackknife-work/"><meta property="og:image" content="https://zh-qifan.github.io/logo.png"><meta property="article:published_time" content="2022-10-08T00:00:00+00:00"><meta property="article:modified_time" content="2022-10-08T00:00:00+00:00"><meta property="og:site_name" content="Qifan Zhang"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zh-qifan.github.io/logo.png"><meta name=twitter:title content="Why does Jackknife method works?"><meta name=twitter:description content="In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows, \begin{equation} b_{JACK} = (n - 1)(\bar{T}_{n-1} - T_n) \end{equation} where $\bar{T}_{n-1} = n^{-1}\sum_{i=1}^n T_{n-1,i} = n^{-1}\sum_{i=1}^n T_{n-1}(X_1,\cdots,X_{i-1},X_{i+1},\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset."><meta name=application-name content="Qifan Zhang"><meta name=apple-mobile-web-app-title content="Qifan Zhang"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://zh-qifan.github.io/posts/why-jackknife-work/><link rel=prev href=https://zh-qifan.github.io/posts/mistake-composition-measurable-function/><link rel=next href=https://zh-qifan.github.io/posts/intuition-jordan-hahn-thm/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Why does Jackknife method works?","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/zh-qifan.github.io\/posts\/why-jackknife-work\/"},"genre":"posts","keywords":"article","wordcount":1459,"url":"https:\/\/zh-qifan.github.io\/posts\/why-jackknife-work\/","datePublished":"2022-10-08T00:00:00+00:00","dateModified":"2022-10-08T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Qifan Zhang"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Qifan Zhang">Qifan Zhang</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>Posts </a><a class=menu-item href=/about/>About </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/categories/>Categories </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item language" title="Select Language"><i class="fa fa-globe" aria-hidden=true></i><select class=language-select id=language-select-desktop onchange="location=this.value;"><option value=/posts/why-jackknife-work/ selected>English</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Qifan Zhang">Qifan Zhang</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/>Posts</a><a class=menu-item href=/about/>About</a><a class=menu-item href=/tags/>Tags</a><a class=menu-item href=/categories/>Categories</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); class=menu-item title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i><select class=language-select onchange="location=this.value;"><option value=/posts/why-jackknife-work/ selected>English</option></select></a></div></div></header><main class=main><div class=container><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Why does Jackknife method works?</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Qifan Zhang</a></span>&nbsp;<span class=post-category>included in <a href=/categories/statistics/><i class="far fa-folder fa-fw" aria-hidden=true></i>statistics</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-10-08>2022-10-08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1459 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;7 minutes&nbsp;</div></div><div class=content id=content><p>In the Advanced Mathematical Statistics course in Spring AY21/22, we studied two popular model-free computational methods used in estimation: Jackknife and Bootstrap. Given $n$ observations $X_1,\cdots, X_n$ and a statistics $T_n$ as the estimation for unknown parameter $\theta$ (e.g. mean value and variance), the Jackknife method for bias estimation formulates as follows,
\begin{equation}
b_{JACK} = (n - 1)(\bar{T}_{n-1} - T_n)
\end{equation}
where $\bar{T}_{n-1} = n^{-1}\sum_{i=1}^n T_{n-1,i} = n^{-1}\sum_{i=1}^n T_{n-1}(X_1,\cdots,X_{i-1},X_{i+1},\cdots,X_n)$ represents the estimator generated by $n-1$ observations in our full dataset. This result was beautiful, since we don&rsquo;t need any additional information even when the true bias of the estimator contains unknow parameter $\theta$. But there is one question: is this simple bias estimator accurate?</p><p>This formula was first proposed by Quenouille in 1949 and the motivation was that suppose the bias $b$ has the expansion<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> (why?)
\begin{equation}
b = \frac{a_1}{n} + \frac{a_2}{n^2} + \cdots
\end{equation}
Then we can verify that by subtracting $b_{JACK}$ from the original $T$, we can reduce the estimation bias from $O(n^{-1})$ to $O(n^{-2})$. In fact, if we consider the case where $T_n=g(\bar{X}_n)$ is used to estimate $\theta=g(\mu)$ and $g$ is sufficiently smooth<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>(e.g. method of moment), we have Taylor expansion
\begin{equation}
T_n - \theta = g^{\prime}(\mu) (\bar{X}_n -\mu) + \frac{1}{2}(\bar{X}_n -\mu)^Tg^{\prime\prime}(\mu)(\bar{X}_n -\mu) + R_n
\end{equation}
where $R_n=O_{p}(n^{-2})$<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> and
$$ E[g^{\prime}(\mu) (\bar{X}_n -\mu)] = 0 $$
$$ E[(\bar{X}_n -\mu)^Tg^{\prime\prime}(\mu)(\bar{X}_n -\mu)] = O(n^{-1}) $$
Thus in this case the expansion $(2)$ is correct.</p><p>Let&rsquo;s further calculate the $b_{JACK}$ in this case. Aagain by Taylor expansion, we have
\begin{equation}
T_{n-1, i} - T_n = g^{\prime}(\bar{X}_n)(\bar{X}_{n-1,i} - \bar{X}_n) + \frac{1}{2}(\bar{X}_{n-1,i} - \bar{X}_n)^Tg^{\prime\prime}(\xi_i)(\bar{X}_{n-1,i} - \bar{X}_n)
\end{equation}</p><p>Then
$$\begin{aligned} b_{JACK} &= (n - 1)(\bar{T}_{n-1} - T_n) \\<br>&= \frac{n-1}{2n}\sum_{i=1}^n (\bar{X}_{n-1,i} - \bar{X}_n)^Tg^{\prime\prime}(\xi_i)(\bar{X}_{n-1,i} - \bar{X}_n) \\<br>&= \frac{1}{2n(n-1)}\sum_{i=1}^n (X_i - \bar{X}_n)^Tg^{\prime\prime}(\xi_i)(X_i - \bar{X}_n) \end{aligned}$$</p><p>where $\bar{X}_{n-1,i}$ is the mean value of $n-1$ observations without the ith one. Since
$\xi_i \rightarrow \mu $ as $n \rightarrow \infty$, we can show that $b_{JACK} \rightarrow E[(\bar{X}_n -\mu)^Tg^{\prime\prime}(\mu)(\bar{X}_n -\mu)]$. By $E[g^{\prime}(\mu) (\bar{X}_n -\mu)] = 0$, we actually prove that the jackknife bias estimator is a consistent estimator of the first two order of the true bias.</p><p>From the above discussion, jackknife bias estimation works well for most estimators i.e. estimators have the form $g(\bar{X}_n)$ as $n\rightarrow \infty$. In fact, for a more general class of statistics named functional statistics<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> (e.g. quantile), we have similar result in the consistency of jackknife bias estimation.</p><p>Jackknife method is popular for its variance estimation, which was first proposed by Tukey in 1958. In fact, the name &ldquo;jackknife&rdquo; was first proposed by Tukey because, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. By doing jackknife bias reduction, i.e. subtracting bias estimation from the original estimator $T_n$, we have
\begin{equation}
T_{JACK} = T_n - (n-1)(\bar{T}_{n-1} - T_n) = \frac{1}{n}\sum_{i=1}^n [nT_n - (n-1)T_{n-1,i}]
\end{equation}
This form is informative since if we set $\tilde{T}_{n,i} = nT_n - (n-1)T_{n-1,i}$, named jackknife pseudovalues, $T_{JACK}$ is the mean value of some new statistics $\tilde{T}_{n,i}$&rsquo;s. Tukey conjectured that $\tilde{T}_{n,i}$&rsquo;s can be treated as though they were i.i.d. This conjecture was verified by Thorburn in 1977<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, who proved that if pseudovalues converge in mean square error to some random variables, these random variables are independent. Note that if we view $T_{JACK}\approx T_n$, $var(T_n)$, if exists, should approximately equal to $var(\tilde{T}_{n,i})/n$. This was actually the second conjecture proposed by Tukey. With this conjecture, Tukey defined the jackknife variance estimator as follows,
\begin{equation}
v_{JACK} = \frac{1}{n(n-1)}\sum_{i=1}^n (\tilde{T}_{n,i} - \frac{1}{n}\sum_{i=1}^n \tilde{T}_{n,i})^2
\end{equation}</p><p>Now let&rsquo;s consider some simple examples. If $T_n = \bar{X}_n$, $T_n$ is unbiased estimator for $\mu$. Then
\begin{equation}
\bar{T}_{n-1} = \frac{1}{n(n-1)}\sum_{i=1}^n (nT_n - X_i) = T_n
\end{equation}
Thus $b_{JACK} = 0$. Further, $v_{JACK}$ is the ordinary variance estimator for $\bar{X}_n$ with bias correction. In this case, the jackknife and the traditional provide the same estimators.</p><p>If we consider a more complicated one<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> where $T_n = \bar{X}_n^2$ as the estimator of $\mu^2$. It can be shown that $E[T_n] = \mu^2 + \sigma^2/n$ and bias$(T_n) = \sigma^2/n$. We expect that in this case, jackknife bias reduction is powerful since bias$(T_n)$ has the form as $(2)$ and $T_n$ is a MM estimator. In fact, the jackknife bias estimation is the same as the plug-in method i.e. replace $\sigma^2$ with its sample form. A straightforward calculation shows that
\begin{equation}
var(T_n) = \frac{4\mu^2\sigma^2}{n} + \frac{4\mu\alpha_3}{n^2} + \frac{\alpha_4}{n^3}
\end{equation}
where $\alpha_k$ denotes the kth central moment. The jackknife variance estimator is
\begin{equation}
v_{JACK} = \frac{4\bar{X}_n^2\hat{\sigma}^2}{n} - \frac{4\bar{X}_n\hat{\alpha}_3}{n(n-1)} + \frac{\hat{\alpha}_4}{n(n-1)^2} - \frac{\hat{\sigma}^4}{n^2(n-1)}
\end{equation}
where $\hat{\alpha}_k$ are the kth sample central moment with bias correction i.e. the denominator is $n - 1$. We notice that $(10)$ and $(11)$ are asymptotically the same and $(10)$ is complicated. This case shows the power and value of jackknife method: we don&rsquo;t need to derive the complicated explicit form of variance like $(10)$. We just need to calculate $(8)$. For statistics like sample quantile, $\alpha$-trimmed sample mean, and some more complicated ones like V-statistics and U-statistics in robust statistics, we can simply use the jackknife method to estimate its bias and variance.</p><p>Now, similar to our discussion in jackknife bias estimation, we need to consider the correctness of jackknife variance estimator. For jackknife variance estimator, we can show that (A) for some classes of statistics, the jackknife variance estimator is consistent; (B) the jackknife variance estimator is almost unbiased or positively biased. We first consider the property (A). Let $T_n = g(\bar{X}_n)$ where g is sufficiently smooth (in fact, we only require it is continuously differentiable). If $\nabla g (\mu) \neq 0$, we have
\begin{equation}
\frac{v_{JACK}}{\sigma_n^2} \rightarrow 1\ \ \ a.s.
\end{equation}
where $\sigma_n$ is the asymptotic variance of $T_n$ (its explicit form can be given by the Delta&rsquo;s method). This indicates that $v_{JACK}$ can be viewed as a variance estimator given by the Delta&rsquo;s method through some simple calculations. In fact, the proof of this relation<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> mainly approximate the $T_n$ by some linear statistics (that&rsquo;s why we assume g is sufficiently smooth with $\nabla g (\mu) \neq 0$), which shares the same idea with the Delta&rsquo;s method.</p><p>Now let&rsquo;s consider the property (B). The relevant result was first obtained by Efron in 1981<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. The main part of the proof is the ANOVA decomposition on estimators. Assume that $T_n$, an estimator for $\theta$, satisfies $ET_n^2 &lt; \infty$. Then we have
\begin{equation}
T_n = \alpha^{(0)} + \frac{1}{n}\sum_{i} \alpha_i^{(1)} + \frac{1}{n^2} \sum_{i_1 &lt; i_2} \alpha_{i_1i_2}^{(2)} + \cdots + \frac{1}{n^n} \alpha_{12\cdots n}^{(n)}
\end{equation}<br>where
$$\begin{aligned} \alpha^{(0)} &= ET_n \\<br>\alpha_{i}^{(1)} &= n[E(T_n|X_i) - \mu] \\<br>\alpha_{i_1i_2}^{(2)} &= n^2[E(T_n|X_{i_1}, X_{i_2}) - E(T_n|X_{i_1}) - E(T_n|X_{i_2}) + \mu] \\<br>&\cdots \\<br>\alpha_{i_1i_2\cdots i_k}^{(k)} &= n^k[E(T_n|X_{i_1}, X_{i_2}, \cdots, X_{i_k}) - \sum_{s=1}^k E(T_n|(X_{i_j})_{j\neq s}) + \\<br>&\ \ \sum_{s,t=1}^k E(T_n|(X_{i_j})_{j\neq s,t}) + \cdots + (-1)^k\mu \end{aligned}$$
and these terms have zero expectation and mutually uncorrelated. This decomposition is powerful since it doesn&rsquo;t assume independence on the original observations. Intuitively, it decomposes an estimator into the main effect, mutual effect and high order effect of observations. One just need to plug in the definition of $\alpha^{(i)}$ into the decomposition to verfify it. Notice that if we assume our observations are i.i.d., we have
\begin{equation}
var(T_{n-1}) = \frac{1}{n-1}\sigma^2_1 + {n-1 \choose 2}\frac{1}{(n-1)^4}\sigma^2_2 + \cdots + \frac{1}{n^{2n}}\sigma_n^2
\end{equation}
where $\sigma_i^2 = var(\alpha^{(i)})$. Calculating the expectation of the jackknife variance estimator using $(13)$ and comparing it with $(15)$, one can show that
\begin{equation}
Ev_{JACK} \geq var(T_{n-1})
\end{equation}
This shows that the &ldquo;bias&rdquo; of the jackknife variance estimator is positive (note that the RHS of the inequality is $T_{n-1}$ not $T_{n}$). For statistics like U-statistics, von Mises Series, e.t.c., we can replace $var(T_{n-1})$ with $var(T_{n})$.</p><p>So far, we have discussed the consistency of the jackknife method and the bias of the jackknife variance estimator. These properties guarantee the jackknife method to be a good method. In fact, our discussion can be extended to functional statistics and delete-d jackknife method (which can be used to estimate the sampling distribution of the $T_n$ like bootstrap but less computationally complex). One can refer to <em>The Jackknife and Bootstrap</em> by Shao, J. and Tu, D..</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Efron, B., 1982. The jackknife, the bootstrap and the other resampling plans. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Shao, J. and Tu, D., 1995. The Jackknife and Bootstrap. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>$O_p(n^{-k})$ here means $n^kO_p(n^{-k})$ is bounded in probability. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>If the true parameter has the form $T(F)$ where $F$ is the population distribution, its estimator can be obtained easily by plug-in method i.e. replace $F$ with empirical distribution $F_n$. We call this statistics functional statistics. <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p><a href=https://en.wikipedia.org/wiki/Jackknife_resampling target=_blank rel="noopener noreffer">Jackknife Resampling</a> <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>Thorburn, D. E., 1977. On the asymptotic normality of the jackknife, Scand. J. Statist., 4, 113-118 <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Efron, B. and Stein, C., 1981. The Jackknife Estimate of Variance. The Annals of Statistics, 9(3). <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-10-08</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/article/>article</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/mistake-composition-measurable-function/ class=prev rel=prev title="A Mistake on Composition of Measurable Function"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>A Mistake on Composition of Measurable Function</a>
<a href=/posts/intuition-jordan-hahn-thm/ class=next rel=next title="An intuitive understanding of the proof for Jordan-Hahn decomposition">An intuitive understanding of the proof for Jordan-Hahn decomposition<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.79.1">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i>LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>Qifan Zhang</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type=text/javascript src=/js/theme.min.js></script></body></html>